# Project Documentation Credit Score Novadrive

This project is an **end-to-end Data Science solution** aimed at predicting customer credit scores to determine whether or not a loan should be granted. It includes credit data analysis using Python tools to generate automated exploratory data analysis (EDA) reports with the **Pandas Profiling** and **Sweetviz libraries**. The data is obtained from a PostgreSQL database, manipulated into a DataFrame, and stored as a CSV file.

## Project Structure

The project directory structure is organized as follows:

```
D:\Repositories\Data-Science\Credit_Score_Novadrive\
|
├── data\
│   └── raw_data\
│       └── data.csv                      # Data exported from the database
|
├── EDA\
│   ├── eda_reports\
│   │   ├── pandas_profiling_report.html  # Report generated by Pandas Profiling
│   │   └── sweetviz_report.html          # Report generated by Sweetviz
│   ├── config.yaml                       # Database configuration file
│   ├── const.py                          # Project settings and paths
│   ├── pandas_profiling_analysis.py      # Script to generate Pandas Profiling report
│   └── sweetviz_analysis.py              # Script to generate Sweetviz report
|
├── queries\
│   ├── data_denormalization.sql          # SQL query for data normalization
│   └── tables_analysis.sql               # SQL query for table analysis
```

## Dependencies

The libraries and tools required for the project include:

- **Python** (>=3.9)
- **Pandas**
- **Sweetviz**
- **Pandas Profiling**
- **PyYAML**
- **psycopg2**
- **PostgreSQL**

Install the dependencies using:

```bash
pip install pandas sweetviz pandas-profiling pyyaml psycopg2
```

## Database Configuration

The `config.yaml` file contains the credentials to access the PostgreSQL database. Make sure to configure it correctly before running the scripts.

Example of `config.yaml`:
```yaml
database_config:
   host: "localhost"
   dbname: "credit_score_db"
   user: "your_user"
   password: "your_password"
```

## File Descriptions

### 1. `const.py`
Defines the main project paths and loads the SQL query from `queries/data_denormalization.sql`.

### 2. `utils.py`
Contains the `fetch_data_from_db` function, responsible for connecting to the database, executing the SQL query, and saving the data as a CSV file.
- **Database connection**: uses the `psycopg2` library to connect to PostgreSQL.
- **Query execution**: executes the provided SQL and converts the result into a Pandas DataFrame.
- **Export to CSV**: saves the data in CSV format for reuse.

### 3. `pandas_profiling_analysis.py`
Generates the exploratory report using **Pandas Profiling** and saves the report in the `eda_reports` directory.
- **Main function**: loads the data from the CSV file and applies the analysis tool to generate the report.

### 4. `sweetviz_analysis.py`
Generates the exploratory report using **Sweetviz** and saves the report in the `eda_reports` directory.
- **Main function**: loads the data, configures the analysis, and exports the report as HTML.

### 5. `queries/data_denormalization.sql`
Contains the SQL query used to fetch and normalize the data from the database.
- **Query structure**: denormalizes relevant tables to facilitate analysis.

## How to Run

1. **Configure the Database**:
    Make sure the `config.yaml` file is correctly configured with the database credentials.

2. **Run the Sweetviz Script**:
    ```bash
    python D:\Repositories\Data-Science\Credit_Score_Novadrive\EDA\sweetviz_analysis.py
    ```

3. **Run the Pandas Profiling Script**:
    ```bash
    python D:\Repositories\Data-Science\Credit_Score_Novadrive\EDA\pandas_profiling_analysis.py
    ```

4. **Check the Reports**:
    - **Sweetviz**: `EDA\eda_reports\sweetviz_report.html`
    - **Pandas Profiling**: `EDA\eda_reports\pandas_profiling_report.html`

## Logs and Debugging

If any error occurs, check the logs generated by `utils.py` and the error messages displayed when running the scripts.

## Future Improvements

- Data Cleaning, Treatment, and Preprocessing for the Model.
- Model Creation and Performance Evaluation.
- Develop Explainability Aspects.
- Create API to Serve the Model.
- Deploy on the Web and Test the API.
- Publish and improve the README.md

## Contact

For questions or suggestions, contact Thiago Alves.