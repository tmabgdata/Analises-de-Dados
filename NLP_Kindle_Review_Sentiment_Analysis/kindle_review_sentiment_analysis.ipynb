{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kindle Review Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview  \n",
    "### Context  \n",
    "This dataset is a small subset of Amazon Kindle Store book reviews.  \n",
    "\n",
    "### Content  \n",
    "The dataset includes 982,619 entries of product reviews from May 1996 to July 2014, where each reviewer and product has at least 5 reviews.  \n",
    "\n",
    "#### Columns:  \n",
    "- **asin**: Unique product ID (e.g., `B000FA64PK`).  \n",
    "- **helpful**: Helpfulness rating of the review (e.g., `2/3`).  \n",
    "- **overall**: Product rating.  \n",
    "- **reviewText**: Full text of the review.  \n",
    "- **reviewTime**: Review timestamp (raw format).  \n",
    "- **reviewerID**: Unique reviewer ID (e.g., `A3SPTOKDG7WBLN`).  \n",
    "- **reviewerName**: Name of the reviewer.  \n",
    "- **summary**: Summary or title of the review.  \n",
    "- **unixReviewTime**: Unix timestamp of the review.  \n",
    "\n",
    "### Acknowledgements  \n",
    "This dataset was sourced from the Amazon product data repository by Julian McAuley (UCSD). Full dataset details are available [here](http://jmcauley.ucsd.edu/data/amazon/).  \n",
    "\n",
    "### Potential Analyses  \n",
    "- **Sentiment Analysis**: Extract and analyze sentiments from review texts.  \n",
    "- **Helpfulness Analysis**: Understand factors influencing review helpfulness.  \n",
    "- **Outlier Detection**: Identify fake or unusual reviews.  \n",
    "- **Product Insights**: Find top-rated products or analyze product similarities based on reviews.  \n",
    "- **Other Explorations**: Discover unique patterns or insights from the data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practises\n",
    "\n",
    "1. Preprocessing And Cleaning\n",
    "2. Train Test Split\n",
    "3. BOW, TFIDF, Word2vec\n",
    "4. Train ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the dataset\n",
    "file_path = r\"D:\\Repositories\\Data-Science\\NLP_Kindle_Review_Sentiment_Analysis\\data\\all_kindle_review.csv\"\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first five rows\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Relevant Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting relevant columns: 'reviewText' and 'rating'\n",
    "df = data[['reviewText', 'rating']]\n",
    "\n",
    "# Display the first five rows of the selected DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Shape of the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the DataFrame (rows, columns)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Unique Values in the Rating Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in the 'rating' column\n",
    "df['rating'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the Frequency of Each Rating Value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each value in the 'rating' column\n",
    "df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing And Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Ratings to Binary Labels (Positive: 1, Negative: 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ratings to binary labels: Positive (1) if rating >= 3, otherwise Negative (0)\n",
    "df['rating'] = df['rating'].apply(lambda x: 0 if x < 3 else 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency of Binary Rating Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of binary labels in the 'rating' column\n",
    "df['rating'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting All Review Text to Lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all text in the 'reviewText' column to lowercase\n",
    "df['reviewText'] = df['reviewText'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Downloading Stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import BeautifulSoup for parsing HTML content\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Review Text\n",
    "- Removing special characters\n",
    "- Removing stopwords\n",
    "- Removing URLs\n",
    "- Removing HTML tags\n",
    "- Removing extra spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing special characters\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s-]+', '', x))\n",
    "\n",
    "# Removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: \" \".join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Removing URLs\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: re.sub(r'(http|https|ftp|ssh)://[\\w\\.-]+(?:/[\\w\\./-]*)?', '', str(x)))\n",
    "\n",
    "# Removing HTML tags\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: BeautifulSoup(x, 'lxml').get_text())\n",
    "\n",
    "# Removing additional spaces\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: \" \".join(x.split()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing: Lemmatization and WordNet Lemmatizer\n",
    "- Initializing WordNet Lemmatizer\n",
    "- Defining Lemmatization Function\n",
    "- Applying Lemmatization on Review Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import WordNetLemmatizer from NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to lemmatize words in the text\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "# Apply the lemmatization function to the 'reviewText' column\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: lemmatize_words(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "- Splitting the dataset into training and testing sets\n",
    "- 80% for training, 20% for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split from scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviewText'], df['rating'], test_size=0.20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW) Transformation\n",
    "- Converting text data into numerical format using CountVectorizer\n",
    "- Training set and test set transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer from scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "bow = CountVectorizer()\n",
    "\n",
    "# Transform the training and testing data using CountVectorizer (Bag of Words)\n",
    "X_train_bow = bow.fit_transform(X_train).toarray()\n",
    "X_test_bow = bow.transform(X_test).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Transformation\n",
    "- Converting text data into numerical format using TfidfVectorizer\n",
    "- Training set and test set transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer from scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Transform the training and testing data using TfidfVectorizer (TF-IDF)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = tfidf.transform(X_test).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Bag of Words Transformation (Training Set)\n",
    "- Displaying the numerical representation of the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Bag of Words transformation of the training data\n",
    "X_train_bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names (words) from the Bag of Words model\n",
    "feature_names = bow.get_feature_names_out()\n",
    "\n",
    "# Convert the matrix to a DataFrame with feature names as columns\n",
    "import pandas as pd\n",
    "X_train_bow_df = pd.DataFrame(X_train_bow, columns=feature_names)\n",
    "\n",
    "# Display the first few rows of the Bag of Words DataFrame\n",
    "X_train_bow_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Naive Bayes Classifier\n",
    "- Training Gaussian Naive Bayes model using Bag of Words (BoW) representation\n",
    "- Training Gaussian Naive Bayes model using TF-IDF representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GaussianNB from scikit-learn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train Naive Bayes model on Bag of Words features (X_train_bow)\n",
    "nb_model_bow = GaussianNB().fit(X_train_bow, y_train)\n",
    "\n",
    "# Train Naive Bayes model on TF-IDF features (X_train_tfidf)\n",
    "nb_model_tfidf = GaussianNB().fit(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation - Naive Bayes (BoW)\n",
    "- Evaluating the Naive Bayes model performance using:\n",
    "  - Confusion Matrix\n",
    "  - Accuracy Score\n",
    "  - Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the prediction is done first\n",
    "y_pred_bow = nb_model_bow.predict(X_test_bow)  # Making predictions with Naive Bayes model\n",
    "\n",
    "# Importing evaluation metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_bow = confusion_matrix(y_test, y_pred_bow)\n",
    "print(\"Confusion Matrix (BoW):\\n\", conf_matrix_bow)\n",
    "\n",
    "# Accuracy Score\n",
    "accuracy_bow = accuracy_score(y_test, y_pred_bow)\n",
    "print(\"\\nAccuracy Score (BoW):\", accuracy_bow)\n",
    "\n",
    "# Classification Report\n",
    "class_report_bow = classification_report(y_test, y_pred_bow)\n",
    "print(\"\\nClassification Report (BoW):\\n\", class_report_bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Score\n",
    "accuracy_bow = accuracy_score(y_test, y_pred_bow)\n",
    "print(\"BoW Accuracy: \", accuracy_bow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation - Naive Bayes (TF-IDF)\n",
    "- Evaluating the Naive Bayes model performance using the Confusion Matrix.\n",
    "- The confusion matrix will help identify the number of correct and incorrect classifications made by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions with Naive Bayes model (TF-IDF)\n",
    "y_pred_tfidf = nb_model_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Importing necessary evaluation metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_tfidf = confusion_matrix(y_test, y_pred_tfidf)\n",
    "print(\"Confusion Matrix (TF-IDF):\\n\", conf_matrix_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TFIDF accuracy: \",accuracy_score(y_test,y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion - Sentiment Analysis Evaluation\n",
    "\n",
    "This project aimed to evaluate sentiment analysis using **Naive Bayes** models, leveraging different text vectorization techniques: **Bag of Words (BoW)** and **TF-IDF**. Both methods provide insights into the effectiveness of these models in classifying sentiment (positive vs. negative reviews). Below are the findings:\n",
    "\n",
    "### **Bag of Words (BoW)**:\n",
    "- **Accuracy Score**: 57.17%\n",
    "- **Confusion Matrix**:\n",
    "  - True Negatives (Negative Reviews): 504\n",
    "  - False Positives (Incorrectly classified as Positive): 309\n",
    "  - False Negatives (Incorrectly classified as Negative): 719\n",
    "  - True Positives (Correctly classified as Positive): 868\n",
    "- **Classification Report**:\n",
    "  - **Precision** for class 1 (positive sentiment) is 0.74, indicating the model is good at identifying positive sentiment when it predicts it, but with some room for improvement in recall (0.55).\n",
    "  - **Recall** for negative reviews (class 0) is 0.62, suggesting the model is better at identifying negative reviews, though still leaving some false negatives.\n",
    "\n",
    "### **TF-IDF**:\n",
    "- **Accuracy Score**: 57.46%\n",
    "- **Confusion Matrix**:\n",
    "  - True Negatives: 494\n",
    "  - False Positives: 319\n",
    "  - False Negatives: 702\n",
    "  - True Positives: 885\n",
    "- **Classification Report**:\n",
    "  - The TF-IDF model showed a slightly higher accuracy than BoW (57.46%), with an improved performance in identifying positive reviews (class 1), though the recall is still lower compared to its precision (0.55).\n",
    "  - The model performed similarly in terms of recall for class 0 (negative sentiment), where the recall was 0.55.\n",
    "\n",
    "### **Insights**:\n",
    "- Both models performed relatively well, achieving accuracy in the range of 57%, but they show a **bias towards identifying negative reviews** (class 0), possibly because of the higher frequency of negative reviews in the dataset.\n",
    "- The **TF-IDF model slightly outperformed BoW** in terms of overall accuracy, which indicates that capturing word importance (via TF-IDF) is slightly more effective for this sentiment analysis task compared to the simple frequency count of BoW.\n",
    "\n",
    "### **Next Steps and Recommendations**:\n",
    "1. **Advanced Models**: Explore more sophisticated models, such as **Logistic Regression**, **Support Vector Machines (SVM)**, or **Gradient Boosting Machines (GBM)**, which may provide better classification performance.\n",
    "2. **Deep Learning Models**: Consider leveraging **LSTM (Long Short-Term Memory)** or **BERT (Bidirectional Encoder Representations from Transformers)** for deeper semantic understanding and improved sentiment classification.\n",
    "3. **Fine-tuning Pretrained Models**: Utilize pre-trained transformer models, such as BERT or RoBERTa, which have shown state-of-the-art results in sentiment analysis tasks and can be fine-tuned for better accuracy.\n",
    "4. **Data Augmentation**: Use techniques such as **back-translation** or **data synthesis** to balance the dataset and improve the model's ability to generalize across both positive and negative sentiments.\n",
    "\n",
    "By incorporating these strategies, the overall performance of the sentiment analysis could be significantly improved, resulting in more accurate and reliable predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Support Vector Machine (SVM) for Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas necessárias\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Treinando o modelo SVM usando dados TF-IDF\n",
    "svm_model = LinearSVC()\n",
    "svm_model.fit(X_train_tfidf, y_train)  # Ajustando o modelo com dados de treino\n",
    "\n",
    "# Realizando previsões no conjunto de teste\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Avaliando o modelo SVM\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "print(\"Confusion Matrix (SVM):\\n\", conf_matrix_svm)  # Matriz de Confusão\n",
    "print(\"\\nAccuracy Score (SVM):\", accuracy_score(y_test, y_pred_svm))  # Acurácia\n",
    "print(\"\\nClassification Report (SVM):\\n\", classification_report(y_test, y_pred_svm))  # Relatório detalhado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of SVM Implementation\n",
    "\n",
    "1. **Model Selection**: We use `LinearSVC`, a computationally efficient version of Support Vector Machines optimized for linear problems.\n",
    "2. **Training**: The `fit` method trains the SVM model using the TF-IDF-transformed training data (`X_train_tfidf`) and labels (`y_train`).\n",
    "3. **Prediction**: The `predict` method generates predictions for the test set (`X_test_tfidf`).\n",
    "4. **Evaluation**:\n",
    "   - **Confusion Matrix**: Summarizes the performance by showing true positives, true negatives, false positives, and false negatives.\n",
    "   - **Accuracy Score**: Measures the proportion of correct predictions.\n",
    "   - **Classification Report**: Provides precision, recall, and F1-score for each class, along with overall metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing LSTM (Long Short-Term Memory) for Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Tokenizing the text\n",
    "# Tokenizer splits text into tokens (words) and creates a numerical representation.\n",
    "tokenizer = Tokenizer(num_words=5000)  # Limit vocabulary to the top 5000 words\n",
    "tokenizer.fit_on_texts(X_train)  # Learn vocabulary from training data\n",
    "\n",
    "# Converting text to sequences of integers\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding sequences to ensure uniform length\n",
    "# Padding ensures all input has the same length, either by adding zeros or truncating.\n",
    "max_length = 100  # Maximum length of each review\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# Creating the LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=100, input_length=max_length),  # Embedding layer\n",
    "    LSTM(128, return_sequences=False),  # LSTM with 128 units\n",
    "    Dropout(0.3),  # Dropout to prevent overfitting\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train_padded, y_train, epochs=5, batch_size=64, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f\"LSTM Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of LSTM Implementation\n",
    "\n",
    "1. **Text Preparation**:\n",
    "   - **Tokenization**: Converts text into a sequence of integers, where each integer represents a word's index in the vocabulary.\n",
    "   - **Padding**: Ensures uniform length for input sequences by truncating or adding zeros to sequences.\n",
    "\n",
    "2. **Model Architecture**:\n",
    "   - **Embedding Layer**: Maps words to dense vectors of fixed size (`embedding_dim` = 100), capturing semantic meaning.\n",
    "   - **LSTM Layer**: Processes sequential data and captures temporal dependencies with 128 memory units.\n",
    "   - **Dropout**: Reduces overfitting by randomly setting some neurons' outputs to zero during training.\n",
    "   - **Dense Layer**: Final layer with a `sigmoid` activation function for binary sentiment classification.\n",
    "\n",
    "3. **Model Compilation**:\n",
    "   - **Optimizer**: `Adam` optimizer adjusts weights to minimize the loss function.\n",
    "   - **Loss Function**: `binary_crossentropy` is used for binary classification tasks.\n",
    "\n",
    "4. **Training and Evaluation**:\n",
    "   - Trains the model for 5 epochs with a batch size of 64.\n",
    "   - Validates the model on the test data during training.\n",
    "   - Evaluates the final performance using accuracy and loss on the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Defining parameters\n",
    "embedding_dim = 100\n",
    "max_features = 5000  # Adjust based on dataset\n",
    "max_len = 300  # Maximum sequence length\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=max_len),\n",
    "    LSTM(128, return_sequences=True, dropout=0.3),\n",
    "    LSTM(64, dropout=0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    validation_data=(X_test_padded, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_padded, y_test, verbose=0)\n",
    "print(f\"LSTM Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_proba = model.predict(X_test_padded)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
