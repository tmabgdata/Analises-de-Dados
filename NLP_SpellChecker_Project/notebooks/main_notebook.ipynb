{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data\n",
    "\n",
    "This step loads the text data from the 'artigos.txt' file. The data is read from the specified file path and the first 500 characters are printed to verify that the content is loaded correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "imagem \n",
      "\n",
      "Temos a seguinte classe que representa um usuário no nosso sistema:\n",
      "\n",
      "java\n",
      "\n",
      "Para salvar um novo usuário, várias validações são feitas, como por exemplo: Ver se o nome só contém letras, [**o CPF só números**] e ver se o usuário possui no mínimo 18 anos. Veja o método que faz essa validação:\n",
      "\n",
      "java \n",
      "\n",
      "Suponha agora que eu tenha outra classe, a classe `Produto`, que contém um atributo nome e eu quero fazer a mesma validação que fiz para o nome do usuário: Ver se só contém letras. E aí? Vou\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Defining the base path of the project\n",
    "BASE_PATH = r'repository_path'\n",
    "\n",
    "# Defining the full path to the 'artigos.txt' file\n",
    "FILE_PATH = os.path.join(BASE_PATH, \"data\", \"raw\", \"artigos.txt\")\n",
    "\n",
    "# Reading the data from the text file\n",
    "with open(FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "# Checking the first 500 characters of the text\n",
    "print(text_data[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Preprocessing and Tokenization\n",
    "\n",
    "In this step, the text data is preprocessed. A function is defined to clean the text by removing non-alphabetic characters, tokenizing the text, removing stopwords (common words that don't add much meaning), and lemmatizing the tokens (reducing them to their base form).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary resources for tokenization and stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define a function for text cleaning and tokenization\n",
    "def clean_and_tokenize(text):\n",
    "    # Remove non-alphabetic characters and convert text to lowercase\n",
    "    text = re.sub(r'[^a-zA-Záàâãéèêíóôõúç]+', ' ', text.lower())\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords (using Portuguese stopwords)\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Split the text into lines and apply the cleaning function\n",
    "processed_texts = [clean_and_tokenize(line) for line in text_data.split(\"\\n\")]\n",
    "\n",
    "# Check the first 5 processed samples\n",
    "processed_texts[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Extraction Using Tfidf\n",
    "\n",
    "This step uses the `TfidfVectorizer` from scikit-learn to transform the tokenized text into numerical features. TF-IDF stands for Term Frequency-Inverse Document Frequency, which helps to evaluate the importance of each word in the corpus. The `stop_words='english'` option removes common English words, but for Portuguese data, this should be adjusted (if necessary).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (39827, 18840)\n",
      "Labels shape: (39827,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert tokenized texts back to strings\n",
    "texts_as_strings = [' '.join(tokens) for tokens in processed_texts]\n",
    "\n",
    "# Extracting features using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')  # 'english' here refers to ignoring common English words\n",
    "X = vectorizer.fit_transform(texts_as_strings)\n",
    "\n",
    "# Placeholder for labels (use actual labels in practice)\n",
    "y = np.random.randint(2, size=X.shape[0])  # Random binary labels for now\n",
    "\n",
    "# Check the shape of the features and labels\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Labels shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Split Data into Training and Test Sets\n",
    "\n",
    "This step splits the dataset into training and test sets. The data is divided into 80% for training and 20% for testing. This helps evaluate the model's performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (31861, 18840)\n",
      "Testing features shape: (7966, 18840)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of the training and test sets\n",
    "print(\"Training features shape:\", X_train.shape)\n",
    "print(\"Testing features shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train Model Using Logistic Regression\n",
    "\n",
    "Here, a logistic regression model is trained using the training data (`X_train`, `y_train`). The trained model is then used to make predictions on the test set. The accuracy of the model is evaluated by comparing the predicted labels with the true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Accuracy: 0.5020085362791865\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Logistic Regression Model Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train Model Using Random Forest\n",
    "\n",
    "In this step, a Random Forest classifier is trained. Random Forest is an ensemble learning method that uses multiple decision trees to make a final prediction. The accuracy of the model is then evaluated on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Accuracy: 0.503138337936229\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model accuracy\n",
    "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "print(f'Random Forest Model Accuracy: {rf_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Hyperparameter Tuning for Random Forest\n",
    "\n",
    "Here, we perform hyperparameter tuning for the Random Forest model using GridSearchCV. This technique searches for the best combination of hyperparameters to improve model performance. After finding the best parameters, the model is re-evaluated on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 20, 'n_estimators': 100}\n",
      "Best Cross-validation Score: 0.5026207845421318\n",
      "Best Random Forest Test Accuracy: 0.5046447401456189\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [10, 20, None],\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display the best parameters and the corresponding accuracy\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "rf_test_accuracy = best_rf_model.score(X_test, y_test)\n",
    "print(f'Best Random Forest Test Accuracy: {rf_test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_SpellChecker_Project-9psT14Fw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
